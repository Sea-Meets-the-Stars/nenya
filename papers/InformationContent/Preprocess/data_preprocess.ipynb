{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "250deea1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.utils import resample\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944ab06d-20e1-4184-851d-9aece230516e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_mnist(target_samples=200000, train_split=150000, valid_split=50000):\n",
    "    \"\"\"\n",
    "    Load MNIST dataset, perform bootstrap resampling, and save to HDF5 format.\n",
    "    \n",
    "    Parameters:\n",
    "    - target_samples: Total number of samples to generate (default: 200,000)\n",
    "    - train_split: Number of training samples (default: 150,000)\n",
    "    - valid_split: Number of validation samples (default: 50,000)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Loading MNIST dataset...\")\n",
    "    # Define transform to convert PIL Image to tensor and then to numpy\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Load the original MNIST dataset using PyTorch\n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root='./data', train=True, \n",
    "        download=True, transform=transform\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root='./data', train=False, \n",
    "        download=True, transform=transform\n",
    "    )\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    x_train_orig = train_dataset.data.numpy()\n",
    "    y_train_orig = train_dataset.targets.numpy()\n",
    "    x_test_orig = test_dataset.data.numpy()\n",
    "    y_test_orig = test_dataset.targets.numpy()\n",
    "    \n",
    "    # Combine training and test sets to have all 70,000 samples\n",
    "    x_all = np.concatenate([x_train_orig, x_test_orig], axis=0)\n",
    "    y_all = np.concatenate([y_train_orig, y_test_orig], axis=0)\n",
    "    \n",
    "    print(f\"Original MNIST dataset size: {x_all.shape[0]} samples\")\n",
    "    print(f\"Image shape: {x_all.shape[1:]} pixels\")\n",
    "    \n",
    "    # Check if we have enough samples or need bootstrap resampling\n",
    "    if x_all.shape[0] >= target_samples:\n",
    "        print(f\"Sufficient samples available. Randomly selecting {target_samples} samples...\")\n",
    "        indices = np.random.choice(x_all.shape[0], size=target_samples, replace=False)\n",
    "        x_resampled = x_all[indices]\n",
    "        y_resampled = y_all[indices]\n",
    "    else:\n",
    "        print(f\"Bootstrap resampling {target_samples} samples from {x_all.shape[0]} original samples...\")\n",
    "        # Bootstrap resampling with replacement\n",
    "        x_resampled, y_resampled = resample(\n",
    "            x_all, y_all, \n",
    "            n_samples=target_samples, \n",
    "            replace=True, \n",
    "            random_state=42\n",
    "        )\n",
    "    print(f\"Resampled dataset size: {x_resampled.shape[0]} samples\")\n",
    "    \n",
    "    # Convert to float32 and normalize to [0, 1] range\n",
    "    x_resampled = x_resampled.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Randomly shuffle the resampled data\n",
    "    indices = np.random.permutation(target_samples)\n",
    "    x_resampled = x_resampled[indices]\n",
    "    y_resampled = y_resampled[indices]\n",
    "    \n",
    "    # Split into training and validation sets\n",
    "    x_train = x_resampled[:train_split]\n",
    "    y_train = y_resampled[:train_split]\n",
    "    x_valid = x_resampled[train_split:train_split + valid_split]\n",
    "    y_valid = y_resampled[train_split:train_split + valid_split]\n",
    "    \n",
    "    print(f\"Training set: {x_train.shape[0]} samples\")\n",
    "    print(f\"Validation set: {x_valid.shape[0]} samples\")\n",
    "    \n",
    "    # Save to HDF5 file\n",
    "    output_filename = 'mnist_resampled.h5'\n",
    "    print(f\"Saving to HDF5 file: {output_filename}\")\n",
    "    \n",
    "    with h5py.File(output_filename, 'w') as f:\n",
    "        # Create datasets with proper shapes: N_images, N_pix, N_pix\n",
    "        f.create_dataset('train', data=x_train, dtype=np.float32, compression='gzip')\n",
    "        f.create_dataset('valid', data=x_valid, dtype=np.float32, compression='gzip')\n",
    "        \n",
    "        # Also save labels (optional, but often useful)\n",
    "        f.create_dataset('train_labels', data=y_train, dtype=np.int32)\n",
    "        f.create_dataset('valid_labels', data=y_valid, dtype=np.int32)\n",
    "        \n",
    "        # Add metadata\n",
    "        f.attrs['description'] = 'MNIST dataset with bootstrap resampling'\n",
    "        f.attrs['total_samples'] = target_samples\n",
    "        f.attrs['train_samples'] = train_split\n",
    "        f.attrs['valid_samples'] = valid_split\n",
    "        f.attrs['image_shape'] = x_train.shape[1:]\n",
    "        f.attrs['data_type'] = 'float32'\n",
    "        f.attrs['normalized'] = 'True (0-1 range)'\n",
    "    \n",
    "    print(f\"Successfully saved HDF5 file with shape:\")\n",
    "    print(f\"  - train: {x_train.shape}\")\n",
    "    print(f\"  - valid: {x_valid.shape}\")\n",
    "    \n",
    "    return output_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17122861",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def verify_hdf5_file(filename):\n",
    "    \"\"\"\n",
    "    Verify the contents of the created HDF5 file.\n",
    "    \"\"\"\n",
    "    print(f\"\\nVerifying HDF5 file: {filename}\")\n",
    "    \n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        print(\"Available keys:\", list(f.keys()))\n",
    "        \n",
    "        # Check datasets\n",
    "        for key in ['train', 'valid']:\n",
    "            if key in f:\n",
    "                data = f[key]\n",
    "                print(f\"{key} dataset:\")\n",
    "                print(f\"  Shape: {data.shape}\")\n",
    "                print(f\"  Dtype: {data.dtype}\")\n",
    "                print(f\"  Min value: {data[:].min():.4f}\")\n",
    "                print(f\"  Max value: {data[:].max():.4f}\")\n",
    "                print(f\"  Mean value: {data[:].mean():.4f}\")\n",
    "        \n",
    "        # Print metadata\n",
    "        print(\"\\nMetadata:\")\n",
    "        for key, value in f.attrs.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "\n",
    "def load_from_hdf5(filename):\n",
    "    \"\"\"\n",
    "    Example function to load data from the created HDF5 file.\n",
    "    \"\"\"\n",
    "    print(f\"\\nExample: Loading data from {filename}\")\n",
    "    \n",
    "    with h5py.File(filename, 'r') as f:\n",
    "        x_train = f['train'][:]\n",
    "        x_valid = f['valid'][:]\n",
    "        y_train = f['train_labels'][:] if 'train_labels' in f else None\n",
    "        y_valid = f['valid_labels'][:] if 'valid_labels' in f else None\n",
    "    \n",
    "    print(f\"Loaded training data: {x_train.shape}\")\n",
    "    print(f\"Loaded validation data: {x_valid.shape}\")\n",
    "    \n",
    "    return x_train, x_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54a6009b-039f-4d84-a7e6-23e584c40cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset...\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 67.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 2.23MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 19.9MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 24.0MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Original MNIST dataset size: 70000 samples\n",
      "Image shape: (28, 28) pixels\n",
      "Bootstrap resampling 200000 samples from 70000 original samples...\n",
      "Resampled dataset size: 200000 samples\n",
      "Training set: 150000 samples\n",
      "Validation set: 50000 samples\n",
      "Saving to HDF5 file: mnist_resampled.h5\n",
      "Successfully saved HDF5 file with shape:\n",
      "  - train: (150000, 28, 28)\n",
      "  - valid: (50000, 28, 28)\n",
      "\n",
      "Verifying HDF5 file: mnist_resampled.h5\n",
      "Available keys: ['train', 'train_labels', 'valid', 'valid_labels']\n",
      "train dataset:\n",
      "  Shape: (150000, 28, 28)\n",
      "  Dtype: float32\n",
      "  Min value: 0.0000\n",
      "  Max value: 1.0000\n",
      "  Mean value: 0.1310\n",
      "valid dataset:\n",
      "  Shape: (50000, 28, 28)\n",
      "  Dtype: float32\n",
      "  Min value: 0.0000\n",
      "  Max value: 1.0000\n",
      "  Mean value: 0.1309\n",
      "\n",
      "Metadata:\n",
      "  data_type: float32\n",
      "  description: MNIST dataset with bootstrap resampling\n",
      "  image_shape: [28 28]\n",
      "  normalized: True (0-1 range)\n",
      "  total_samples: 200000\n",
      "  train_samples: 150000\n",
      "  valid_samples: 50000\n",
      "\n",
      "Example: Loading data from mnist_resampled.h5\n",
      "Loaded training data: (150000, 28, 28)\n",
      "Loaded validation data: (50000, 28, 28)\n",
      "\n",
      "Preprocessing complete!\n",
      "HDF5 file saved as: mnist_resampled.h5\n",
      "File size: 56.10 MB\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Main preprocessing\n",
    "    output_file = load_and_preprocess_mnist(\n",
    "        target_samples=200000,\n",
    "        train_split=150000,\n",
    "        valid_split=50000\n",
    "    )\n",
    "    \n",
    "    # Verify the created file\n",
    "    verify_hdf5_file(output_file)\n",
    "    \n",
    "    # Example of loading the data back\n",
    "    x_train, x_valid, y_train, y_valid = load_from_hdf5(output_file)\n",
    "    \n",
    "    print(f\"\\nPreprocessing complete!\")\n",
    "    print(f\"HDF5 file saved as: {output_file}\")\n",
    "    print(f\"File size: {os.path.getsize(output_file) / (1024**2):.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116a0d9e-816b-476a-a83c-3f95a2317b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
